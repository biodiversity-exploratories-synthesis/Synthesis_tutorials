---
title: "Mixed Effects Models Cheatsheet"
author: "N. Schenk"
date: "2022-11-17"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Mixed Effects models

## Aim

This is not a real tutorial but rather a cheatsheet to quickly look up things you already learned but keep forgetting:)

## dependencies
Packages
```{r}
library(nlme) # used in Zuur et al. primarily
library(lme4) # other package for mixed effects models

# helper packages
library(car) # for qqPlot function
library(emmeans) # copute contrats for multilevel factors
library(ggeffects) # produce nice marginal plots
library(sjPlot) # help with visualisations
# library(glmmTMB) # required by sjPlot, but takes long time to install (skipped here)

# AED is the package accompanying the Book from Zuur et al.
# install.packages("remotes")
# remotes::install_github("romunov/AED")
library(AED)

# overview table
tab1_compare_lme4_nlme <- read.table("table1_compare_lme4_nlme.csv", sep = ";", header = T, encoding = "UTF-8")
# note, if this line does not work, try "Tutorials/..." as file name

# # Function for plotting residuals
# #
# # creates a Tukey-Anscombe plot and a QQPlot
# # Tukey-Anscombe-plot : check heterogeneity of residuals
# # QQPlot : check normality of residuals
# library(car)
# plotresid <- function(lmodel) {
#   # plots the residuals of a linear model; requires loading package car
#   par(mfrow = c(1, 2))
#   plot(fitted(lmodel), resid(lmodel), xlab = "Fitted values", ylab = "Residuals", 
#        main = "TA plot")
#   qqPlot(resid(lmodel), dist = "norm", mean = mean(resid(lmodel)), sd = sd(resid(lmodel)),
#          xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
#          main = "Q-Q plot of residuals")
# }

```

Datasets
```{r}
data("RIKZ")     # from the AED package
data("Machines") # from the nlme package
```


## Resources
- Book : Zuur, A., Ieno, E. N., Walker, N., Saveliev, A. A., & Smith, G. M. (2009). Mixed effects models and extensions in ecology with R (2009th ed.). Springer.
- nice Tutorial : https://ourcodingclub.github.io/tutorials/mixed-models/ (thanks Marta for finding)
- Tutorial about factors (incl. mixed effects models) : http://courses.atlas.illinois.edu/spring2016/STAT/STAT200/RProgramming/RegressionFactors.html

# Packages for mixed effects modelling

```{r, echo = F, results = "asis"}
knitr::kable(tab1_compare_lme4_nlme, caption = "Note that the `&#124;` character is pipe |")
#TODO check correct encoding of pipe for rmarkdown.
```


# marginal F tests for fixed effects : 

```{r}
library(nlme)
library(lme4)
library(lmerTest)

data(sleepstudy)
geno <- sample(as.factor(c("A", "B", "C")), size = nrow(sleepstudy), replace = T)
sleepstudy <- cbind(sleepstudy, geno)
fm1 <- lmer(Reaction ~ Days + geno + (1 | Subject), sleepstudy)
anova(fm1, type = "marginal")
```


# Simple examples

## Random Intercept
Using the RIKZ dataset.
```{r}
# using the package nlme
RIKZ$Beach <- factor(RIKZ$Beach)
Mlme1 <- lme(Richness ~ NAP, random = ~ 1 | Beach, data = RIKZ)

summary(Mlme1)
```


## Random Intercept and Slope
Using the Machines dataset
```{r}
machines.lme.2 <- lme(score ~ Machine, random = ~ 1 | Worker/Machine, data=Machines)
```



# Checking Assumptions
It is crucial to check the model assumptions after fitting. Mixed effects model assumptions concern the error terms $\epsilon$ and the random effects $b_i$.

Assumptions on **error terms** : The residuals within groups should be i.i.d. normally distributed, with mean 0 and a common variance, and they should be independent of the random effects. To check this, we use the "raw" residuals within groups = The differences between the observed and fitted values within groups. (use the function `residuals()``)

Assumptions on random effects : Random terms should be normally distributed with mean == 0 and same variance, and they should be independent across groups. Get the predictions of random effects with `ranef()`

*Note* that different sources recommend checks on different degrees of detail. Sources used : Zuur et al. and (internal slides from) Course on Mixed Effects Models, University of Bern, by Dr. M. Vock


## General model fit
- check overall model fit : plot the observed against the predicted values of the given model. Ideal case : All points lie on the diagonal.
- check homogeneity of residuals per (explanatory) variable. Ideal case : no pattern nor in points or in the variance.
    - If pattern or pattern in spread : wrong model --> add more explanatory variables, interactions, quadratic terms or use additive mixed modelling

```{r}
# check overall model fit
plot(fitted(Mlme1), RIKZ$Richness)
abline(0, 1)
# ideal case : all points lie on the diagonal
# comment : deviation in larger values. (The model fit is not good for large values.)

# Homogeneity of residuals (Tukey-Anscombe Plot)
# Check residual vs. individual explanatory variables
Res <- resid(Mlme1, type = "normalized")
Fit <- fitted(Mlme1)
# for categorical explanatory variables (example invents a variable catvar)
# boxplot(Res ~ catvar, data = RIKZ, main = "catvar", ylab = "Residuals")
# for continuous explanatory variables
plot(x = RIKZ$NAP, y = Res, ylab = "Residuals", xlab = "NAP")
abline(h = 0, lty="dotted")
# note : do this for each explanatory variable
# ideal case : all points are equally spread around the horizontal line. No pattern in the points or in the variance of the points visible.
```

## Check Assumptions on Error terms within groups

Plot **residuals per group** (per random effect group)
```{r}
# is the mean == 0 and variance equal?
boxplot(residuals(Mlme1) ~ RIKZ$Beach, ylab="Residuals")
abline(h = 0, lty = "dotted")
# note : do for each random term (i.e. for each group)
# ideal case : all means are at 0 and the variance is the same for each group.
```

*Additional* : Plot residuals against predicted values per random effect group.
```{r}
plot(Mlme1, resid(.) ~ fitted(.) | Beach, grid = FALSE, abline = 0, lty = "dotted")
# ideal case : no pattern of the points nor the variance of the points.
```

*Additional* for hierarchical models (random intercept and slope): Plot residuals against each continuous explanatory variable per random effect combination.
```{r}
#TODO see MEM script ex. 3
```

QQPlot of the **residuals across groups and within group**.
```{r}
# normality of residuals across groups
qqPlot(resid(Mlme1), dist = "norm", mean = mean(resid(Mlme1)), sd = sd(resid(Mlme1)),
         xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
         main = "Q-Q plot of residuals")
# ideal case : all points are near to the diagonal and within the blue shades

# normality of residuals within group
#  for Beach 1
qqPlot(resid(Mlme1)[RIKZ$Beach == 1], dist = "norm", mean = mean(resid(Mlme1)), sd = sd(resid(Mlme1)),
         xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
         main = "Q-Q plot of residuals")
#  for Beach 2
qqPlot(resid(Mlme1)[RIKZ$Beach == 2], dist = "norm", mean = mean(resid(Mlme1)), sd = sd(resid(Mlme1)),
         xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
         main = "Q-Q plot of residuals")
#  continue for each group in the random term
```


## Check Assumptions on Random terms

QQPlot of random intercepts on each level (if you have more than 1 random effect, see example below).
```{r}
# Do the levels of the given random effect come from the same normal distribution?
qqPlot(ranef(Mlme1,level=1)[,1], dist = "norm",
       mean = mean(resid(Mlme1)), sd = sd(resid(Mlme1)),
         xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
         main = "Q-Q plot of across-group residuals")

# if more than 1 level of random effects
# qqPlot(ranef(Mlme1,level=2)[,1], dist = "norm",
#        mean = mean(resid(Mlme1)), sd = sd(resid(Mlme1)),
#          xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
#          main = "Q-Q plot of across-group residuals")
```

*Additional* if you have >1 random effect : Plot random intercept on each level (TODO).
```{r, eval = F, echo = F}
#TODO
# mean == 0 and equal variance
re <- ranef(Mlme1, augFrame=TRUE)

# add names to df
re$Block <- factor(substring(rownames(re), 1, regexpr(’/’, rownames(re))-1))
re$Variety <- factor(substring(rownames(re), regexpr(’/’, rownames(re))+1, nchar(rownames(re))))

# plot
with(re, plot(as.numeric(Block), ‘(Intercept)‘, xlab="Block",
main="Gruppierung nach Block"))
abline(h=0, v=1:6, lty=’dotted’, col=’grey’)
with(re, plot(as.numeric(Variety), ‘(Intercept)‘, xlab="Variety",
main="Gruppierung nach Hafersorte"))
abline(h=0, v=1:3, lty=’dotted’, col=’grey’)
```



## Minimal example
The `sjPlot` package can be used to generate the most important diagnostic plots. Note that the last plot can only be done if the mixed effects model has been fitted with the `lme4` package!

Please additionally create the plot for General model fit, to see if observed and predicted values correspond to each other (see above).
```{r}
Mlme1_lme4 <- lme4::lmer(Richness ~ NAP + (1 | Beach), data = RIKZ)

# Is a lineaer model the right choice? Check if all coefficients are modelled well with a line. Plots the slopes (coefficients) of each predictor (=explanatory variable) against the response.
# downside : does not show individual data points, but a blue line. Check if the blue line is within the red confidence intervals or if there is a deviation
sjPlot::plot_model(Mlme1_lme4, type = "slope")

# Check Tukey-Anscombe plot per predictor (downside: does not show individual points, as above)
sjPlot::plot_model(Mlme1, type = "resid")

# Checking the model assumptions
sjPlot::plot_model(Mlme1, type = "diag") # (1) QQplot (2) similar to QQPlot (3) Tukey-Anscombe Plot

# Plotting the random effects (only works for models from the lme4 package)
# Do the residuals within random effect levels have the same variance and mean == 0?
# sjPlot::plot_model(Mlme1_lme4, type = "re")
# please not that for the above plot, the package glmmTMB needs to be installed
```


<!-- ## TODO CLEAN Error term assumptions -->
<!-- check constant variance (homoscedasicity) and mean = 0 across groups (across groups within the random effects) -->
<!-- plot(Mlme1, Beach ~ residuals(.), abline=0) -->

<!-- - across groups :  -->
<!--     - check homogeneity of residuals : plot residuals against fitted values -> identify violation of homogeneity. Ideal case : points fluctuate randomly around a horizontal line through zero. Violation is indicated by differences in spread of the residuals, e.g. increase in spread for larger fitted values. Method : Tukey-anscombe Plot -->
<!--         - if spread increases with larger fitted values -> (i) apply transformation to response (ii) check if increase in spread is due to a covariate (iii) use generalised linear modelling (e.g. with poisson distr. for counts) -->
<!--         - Additional : plot residuals against each explanatory variable -> check for patterns in spread. -->
<!--             - if pattern is found : add more explanatory variables, interactions, quadratic terms, and if all that does not help: use additive mixed modelling -->
<!--     - Normality of residuals : all residuals within groups should come from the same normal distribution. Method : QQplot (graphic tools recommended, same as in linear models) -->


<!-- - within groups :  -->
<!--     - Plot differences between observed and predicted values within groups. Method : boxplot or Tukey-Anscombe-Plot -->
<!--         - Additional : plot Residuals within groups against individual predicted values of a given group variable (e.g. if a variable is "gender", plot for "masculine", "feminine" and "diverse" separately). -->





<!-- ```{r, eval = F} -->
<!-- # ACROSS GROUPS -->
<!-- # -->
<!-- # heterogeneity of residuals across groups -->
<!-- plot(fitted(Mlme1), resid(Mlme1), xlab = "Fitted values", ylab = "Residuals",  -->
<!--        main = "TA plot") -->
<!-- # Check residuals vs. explanatory -->
<!-- plot(residuals(Mlme1), RIKZ$NAP) -->
<!-- # -->
<!-- # normality of residuals across groups -->
<!-- qqPlot(resid(Mlme1), dist = "norm", mean = mean(resid(Mlme1)), sd = sd(resid(Mlme1)), -->
<!--          xlab = "Theoretical quantiles", ylab = "Empirical quantiles", -->
<!--          main = "Q-Q plot of residuals") -->
<!-- # qqnorm(resid(Mlme1)) # alternative command -->
<!-- # note that the assumptions are violated. -->


<!-- # WITHIN GROUPS -->
<!-- # -->
<!-- # Plot differences between observed and predicted values within groups. Method : boxplot or Tukey-Anscombe-Plot -->
<!-- # mean == 0 and equal variance -->
<!-- re <- ranef(Mlme1, augFrame=TRUE) -->
<!-- boxplot(re$`(Intercept)` ~ re$Sample) -->
<!-- # continue for each random effect -->
<!-- ``` -->






<!-- **Error term assumptions** -->

<!-- - Compare observed and predicted values -->
<!-- - Check normal distribution with QQplot -->
<!-- - Check mean == 0 and constant variance with boxplot -->



<!-- - for random effects : ...#TODO -->


<!-- using the random intercept example from above, `Mlme1` -->
<!-- ```{r, eval = F} -->
<!-- # Random term assumptions -->
<!-- # -->
<!-- # Normality -->
<!-- qqnorm(Mlme1, ~ ranef(.)) -->
<!-- # -->
<!-- # mean == 0 and equal variance -->
<!-- re <- ranef(Mlme1, augFrame=TRUE) -->
<!-- boxplot(re$`(Intercept)` ~ re$NAP) -->
<!-- # continue for each random effect -->




<!-- # Solution : transformation of response variable -->
<!-- Mlme1t <- lme(sqrt(Richness) ~ NAP, random = ~ 1 | Beach, data = RIKZ) -->
<!-- plotresid(Mlme1t) -->

<!-- ``` -->













# Transformation
If the assumption of homogeneity is violated, a transformation can help. Here, square root transformation. Remember to always check the model assumptions also after transformation (not only before).

Often used transformations
- logarithmic (base 2, sometimes base 10) --> small values close to each other are spread, large and spread values are brought closer together
- square root
- for explanatory variables : adding a quadratic term

# Transformation of an explanatory variable
- if the only or the dominant problem is non-linearity (independence, normality and equal variance is met)
- if in a multiple model, only one of the explanatory variables has a non-linearity issue

# Transformation of the response
- if transformation of the explanatory variable does not help
- if your response is far from normally distributed
- if multiple problems arise during model validation
```{r}
qqPlot(resid(Mlme1), dist = "norm", mean = mean(resid(Mlme1)), sd = sd(resid(Mlme1)),
         xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
         main = "Q-Q plot of residuals")
# Solution : transformation of response variable
Mlme1t <- lme(log(Richness + 6) ~ NAP, random = ~ 1 | Beach, data = RIKZ)
qqPlot(resid(Mlme1t), dist = "norm", mean = mean(resid(Mlme1t)), sd = sd(resid(Mlme1t)),
         xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
         main = "Q-Q plot of residuals") # plots look way better
```




# Specific cases

## Post-hoc tests for multilevel factors

keywords : Omnibus test, contrasts, pairwise comparisons

Aim : compare the levels of a factor pairwise. In the Machines example : which machine differs from which machine?
```{r}
library(nlme)
library(emmeans)
library(ggplot2)
library(ggeffects)

data("Machines") # from the nlme package
machines.lme.1 <- lme(score ~ Machine, random = ~ 1 | Worker, data = Machines)
emmeans(machines.lme.1, pairwise ~ Machine, adjust="bonferroni")
ggpredict(machines.lme.1, c("Machine")) |> plot()
```



# Plotting

ggeffects package: ...will add text... 


## Note on different packages

A colleague in our lab was investigating the use of the ggeffects package for visualisation of mixed model results. He spent quite some time investigating this, and therefore wanted to share with us, as many of us are using mixed effects models.
While the effects package does what it should, it uses gridplots for visualisation. These plots are a bit more limited in communicating the results as not so well thought-through/ designed as the ggplots. Alternatively, there was the remef package, which was not on CRAN.
There is a new package, ggeffects, which can visualise the effects package predictions in a ggplot. But there are 3 different functions in this package for visualisation, which differ in the handling of factors :

- ggpredict() uses analogous theory to the predict() function in base R. For predictions, it uses the **first level of each factor**.
- ggemmeans() uses analogous theory to the emmeans package, and takes average of factor levels
- ggeffects() uses analogous theory to the effects package and takes weighted averages of the factor levels.

Our colleague's choice is the `ggeffects` package, as it relies on the effect package philosophy which seems a good choice to him, and is what we currently understand best.


## Plotting results with ggeffects

```{r}
ggpredict(machines.lme.1, c("Machine")) |> plot()
# adding data points
ggpredict(machines.lme.1, c("Machine")) |> plot(add.data = T)


# alternative way to code : 
me.plotmachines1 <- ggpredict(machines.lme.1, c("Machine"))
plot(me.plotmachines1)

# adding data points
plot(me.plotmachines1, add.data = T)
```



### note on marginal effects
The type argument in the ggpredict() function of the ggeffects package determines whether the plotted marginal effects are fixed or random effects.

When type = "fixed", the marginal effects are calculated using the average values of all the random effects in the model, "excluding the additional variance caused by the random effect". This means that the marginal effects represent the average effect of the predictor variable on the response variable, while holding all other variables constant at their mean values. In other words, the marginal effects reflect the effect of the predictor variable on the response variable in the population, assuming that all other variables are held constant at their mean values.

On the other hand, when type = "random", the marginal effects are calculated using the individual values of the random effects in the model. This means that the marginal effects represent the effect of the predictor variable on the response variable for each individual or subgroup in the data, taking into account the individual differences in the random effects. This can be useful when investigating the extent to which the effect of the predictor variable varies across different subgroups or individual cases in the data.




Plotting more than one effect
```{r}
Mlme2 <- lme(Richness ~ NAP * Exposure, random = ~ 1 | Beach, data = RIKZ)
ggpredict(Mlme2, c("NAP", "Exposure")) |> plot()
ggpredict(Mlme2, c("NAP", "Exposure")) |> plot(add.data = T)
```



With the `sjPlot` package, interactions can be plotted easily as well: 
```{r}
# fit a model with an interaction
RIKZ$Exposure <- as.factor(RIKZ$Exposure)
Mlme2 <- lme(Richness ~ NAP * Exposure, random = ~ 1 | Beach, data = RIKZ)
summary(Mlme2)

# plot a model with an interaction
sjPlot::plot_model(Mlme2, type = "pred", terms = c("NAP", "Exposure"))
# plot all interactions with type = "int"
sjPlot::plot_model(Mlme2, type = "int", terms = c("NAP", "Exposure"))
```





